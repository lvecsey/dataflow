
Each chunk of data that is spread around through the dataflow approach
has an optional key string that can be used to look up the data chunk.
There is also one or more Backup-Host that can be specified as either an
ip address or a hostname. The idea is that you would keep appending additional
backup hosts, and rarely purge or remove them unless there is some confidence
that it isn't needed anymore.

Content-Length is how long the chunk of data is.
Cache time is measured in seconds. The same file can be served through the cache life of the dataflow chunk.
TTL refers to the time to live of the data flow header being described, including the host and backup-host entries.
md5hash is the hash of the data chunk and can also be used as a key of sorts.

Note: It is up to the dfserve instance running on the target (upload) machine, to publish for example a json file of all the content-length's and md5sum's that have been uploaded. There is no confirmation given to the client that the upload has succeeded; it is done completely out-of-band.

Before running dfserve, you can change the current working directory to the storage location such as /mnt/dfstor or an area such as ~/public_html/dfstor

When pushing files to your dfserve host you will want to push it to all the backup (dfserve) hosts as well.

There is a way to tell the dfserve that you are talking to, to replicate data to a backup host. Once you inform it of other dfserve processes to talk to it will do this as you upload your data. This is convenient because you just have to send the data once, and it can be replicated (locally) on the other side to many additional servers.

In this example, replicate_list.txt contains ipaddress:port pairs on each line.
There is also a TARGET line specified, to separate another set of ipaddress:port pairs that will be used deeper in the communication process. Each dfserve process will connect to a set of host pairs and then strip those away, sending the remaining list onwards.

    ./genrep < replicate_list.txt > repliace_list.dat

You can feed the binary encoded replicate_list.dat file to pushdf.

    export REPLICATE_FN=./replicate_list.dat
    tcpclient -vRHl0 ip.ad.re.ss port ./pushdf < uploadfile.dat







